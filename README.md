# Greek question answering BERT approach

Design of a greek question-answering NLP system based on pretrained bert model

The bert model used for finetuning and the tokenizer are from the following source 
https://github.com/nlpaueb/greek-bert

The dataset consists of pairs of questions and answers gathered on the web.
The model was finetuned using bert at "question-answering" mode.

Part of general training of bert is next sentence prediction.

This task is performed in order to learn connections between sentences. From the available text corpora, sentence pairs are formed. Whenever the pairs are subsequent in the original text, a IsNext label is attached to them. Whenever the sentence pairs are not subsequent in the original texts, a NotNext label is attached.

Based on that principle, question-answering models can be generated by finetuning bert on a relatively small dataset of question-answer pairs.

![alt text](https://github.com/hariSky/Greek_question_answering/blob/main/imgs/bertimgs.png)



The performance of the developed system can be seen on the examples bellow

Paragraph:
>Έγιναν επίσης ναυτικές επιχειρήσεις αντιπερισπασμού και παρενόχλησης προκειμένου να μην επέμβει το Γερμανικό Ναυτικό στις περιοχές που θα γινόταν η απόβαση, ενώ η θάλασσα της Μάγχης αποναρκοθετήθηκε από τις γερμανικές νάρκες και ποντίσθηκαν νέες. Στην επιχείρηση έλαβαν μέρος πάνω από τρία εκατομμύρια άνθρωποι, από τους οποίους οι 195.700 ήταν το προσωπικό των συμμαχικών σκαφών. Η απόβαση έλαβε χώρα στη χερσόνησο Κοταντέν, την ανατολική όχθη του ποταμού Ορν και τον κόλπο του Σηκουάνα, σε πέντε παραλίες με τα κωδικά ονόματα Γκολντ (Gold), Τζούνο, (Juno), Όμαχα (Omaha), Σουόρντ (Sword) και Γιούτα (Utah). Θεωρήθηκε ότι ολοκληρώθηκε στις 30 Ιουνίου 1944, όταν πια είχε εδραιωθεί το προγεφύρωμα των Συμμάχων. Μέχρι τότε είχαν συνολικά μεταφερθεί 850.000 άνδρες, 148.000 οχήματα και 570.000 τόννοι προμηθειών στις γαλλικές ακτές από 7.000 πλοία, από τα οποία μόλις 59 βυθίστηκαν

Question:
>Που έγινε η απόβαση?

Answer:
>στη χερσονησο κοταντεν



Question:
>Πόσα πλοία έχασαν οι σύμμαχοι?

Answer:
>59


# Training

The training script is the greek_qa_bert.ipynb.
The dataset used is a translated part of the SQUAD dataset.
The training script performs preprocessing to extract the answer part from the transpated text.
In terms of training data, I have pushed one of the QA xlsx files 'result_df_with_sentence_20000.xlsx'
Other similar files can be created with the same fashion and be merged in order to get better results.